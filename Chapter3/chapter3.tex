%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Data Quality (DQ) Evaluation}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\section{Metrics and Measurement}
Any data can have its quality measured. Using a data driven strategy, the measurements acts on the data itself to quantify the DQD (Data Quality Dimension). 
As mentioned before, our work is based on structured data represented in \cite{Juddoo} a set of attributes, columns, and rows with their values. Any data quality metric should specify whether the values of data respect or not the quality attributes. In, the author quoted that data quality measurement metrics tend to evaluate a
binary results correct or incorrect or a value between 0 and 100, and use universal formulas to
compute these attributes. This will apply to many quality dimensions such as accuracy, completeness, and consistency.
 
\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{clccc}
			\multicolumn{1}{l}{}                                                                    &                                                                                                                                & \multicolumn{1}{l}{}                                           & \multicolumn{1}{l}{}                                               & \multicolumn{1}{l}{}                                              \\ \cline{3-5} 
			\multicolumn{1}{l}{}                                                                    & \multicolumn{1}{l|}{}                                                                                                          & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Data Quality Dimensions Related}}                                                                                                                   \\ \cline{2-5} 
			\multicolumn{1}{c|}{}                                                                   & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{Data Quality Issues}}                                                      & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Accuracy}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Completeness}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Consistency}} \\ \hline
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{Missing Data}}                                                                                     & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{X}                                             & \multicolumn{1}{c|}{}                                             \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{Incorrect data, Data entry errors,}}                                                               & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{}                                              & \multicolumn{1}{c|}{}                                             \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{Irrelevant data}}                                                                                  & \multicolumn{1}{c|}{}                                          & \multicolumn{1}{c|}{}                                              & \multicolumn{1}{c|}{X}                                            \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{Outdated data}}                                                                                    & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{}                                              & \multicolumn{1}{c|}{}                                             \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\multirow{-5}{*}{\cellcolor[HTML]{EFEFEF}\textbf{Instance Level}}} & \multicolumn{1}{l|}{\textit{Misfiled and Contradictory values}}                                                                & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{X}                                             & \multicolumn{1}{c|}{X}                                            \\ \hline
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{\begin{tabular}[c]{@{}l@{}}Uniqueness constrains, Functional\\ dependency violation\end{tabular}}} & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{}                                              & \multicolumn{1}{c|}{}                                             \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}}                                          & \multicolumn{1}{l|}{\textit{Wrong data type, poor schema design}}                                                              & \multicolumn{1}{c|}{}                                          & \multicolumn{1}{c|}{}                                              & \multicolumn{1}{c|}{X}                                            \\ \cline{2-5} 
			\multicolumn{1}{|c|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}\textbf{Schema Level}}}   & \multicolumn{1}{l|}{\textit{Lack of integrity constraints}}                                                                    & \multicolumn{1}{c|}{X}                                         & \multicolumn{1}{c|}{X}                                             & \multicolumn{1}{c|}{X}                                            \\ \hline
		\end{tabular}%
	}
	\caption{Data Quality Issues vs DQD}
	\label{tab:my-table}
\end{table}

The DQDs (Data Quality Dimensions) must be relevant to the DQ problems as identified In Table 3.1 Therefore DQ Metrics are 
designed for each DQD to measure if the attributes respect the previously defined DQD. These measures
are done for each attribute given it type, data ranges values, and if it is collected from data profiling.

For example a metric that calculates the accuracy of a data attribute is defined as follows:

\begin{itemize}
  \item{
    The data type of an attribute and its values.}
  \item{
    For numerical attributes a range or sets of acceptable values (Textual also) are defined.
    Any other values are incorrect.
  }
  \item {
	The accuracy of an attribute is calculated based on the number of correct values divided by number of observations or rows.
  }
  \item{
	  For another data types/formats like images, videos, audio files, another type of metrics must be defined to evaluate
	  accuracy or any other quality dimensions. The authors of ~\cite{Firmani2015} describe usefulness as an aspect of data quality
	  for images. For this kind of data, feature extraction functions are defined on the data and extracted 
	  for each data item. These features have constraints that characterize the goodness or badness 
	  of data values. Some of quality metrics functions are designed based on the extracted features such as, usefulness, accuracy
	  , completeness and any other data quality dimensions judged by domain experts to be candidate for 
	  such data type. 
  }
\end{itemize}

\section{DQ Issues and Big Data Characteristics}

Data characteristics commonly named V's are initially, Volume, Velocity, Variety, and Veracity. Since the Big Data inception; 
we reached now 7 V's and probably we will keep going ~\cite{Gupta}. The veracity tends more to express and describe
trust and certainty of data that can be expressed mostly as quality of the data. 
The DQD accuracy is often related to precision, reliability and veracity ~\cite{Laboisse}.

A mapping tentative between these characteristics, data and data quality is complied in ~\citep{Caballero} ~\cite{Firmani2015} ~\cite{Zhu}.
The authors attempted to link the V's to the quality dimensions.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{|c|l|}
	\hline
	\rowcolor[HTML]{EFEFEF} 
	\textbf{DQ Dimensions}             & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Metric functions}} \\ \hline
	\multicolumn{1}{|l|}{Accuracy}     & \multicolumn{1}{c|}{Acc = ( Ncv / N )}                                 \\ \hline
	\multicolumn{1}{|l|}{Completeness} & \multicolumn{1}{c|}{Comp = ( Nmv / N )}                                \\ \hline
	\multicolumn{1}{|l|}{Consistency}  & \multicolumn{1}{c|}{Cons = ( Nvrc / N )}                               \\ \hline
	Ncv                                & Number of correct values                                               \\ \hline
	Nmv                                & Number of missing values                                               \\ \hline
	Nvrc                               & Number of values that respects the constraints                         \\ \hline
	N                                  & Total number of values (rows) of the sample Dataset                    \\ \hline
	\end{tabular}%
	}
\caption{DQD metric functions}
\label{tab:my-table}
\end{table}

\begin{figure}[h]
	\vspace*{.1in}
	\hspace*{-.7in}
	\centering
	\includegraphics[scale=.55]{big_data_quality_evaluation_scheme}
	\caption{Big Data Quality Evaluation Scheme}    
\end{figure}

\section{Big Data Quality Evaluation Scheme}

The purpose of Big Data Quality Evaluation (BDQ) Scheme is to address the data quality before starting data analytics.
This is done by estimating the quality of data attributes or features by applying a DQD metric to measure the quality characterized by its
accuracy, completeness or/and consistency. The expected result is data quality assessment suggestions indicating the quality 
constraints that will increase or decrease the data quality.

The BDQ Evaluation scheme is illustrated in Figure 3.1 where the data goes through many module to estimate its quality.
The key modules of our scheme consist of: (a) data sampling; and data profiling, (b) DQD vs attributes selection, 
(c) data quality Metric selection, (d) samples data quality evaluation. In the following sections, we describe each module, its input(s), output(s),
and the main functions.

\subsection{Big Data Sampling}

A sample is representative of a whole population. Based on a sample, we make several decision about a population. 
A sample is also called a subgroup. The number of observations or units in a sample is called sample size. 
The number of times a sample is collected is usually referred to as the sampling frequency. In designing a control chart, we must 
specify both of these parameters. ~\cite{Jugulum14}

There are several sampling strategies that can be applied on Big Data as expressed in ~\cite{Asilomar} ~\cite{SIGKDD}.
They evaluated the effect of sampling methods on Big Data and believed that sampling large datasets reduces run time and computational footprint of link 
prediction algorithms though maintaining sufficient prediction performance. In statistic, Bootstrap sampling technique evaluates the sampling distribution of an 
estimator by sampling with replacement from the original sample. In the context of Big Data, Bootstrap sampling has been addressed in many works 
~\cite{Liang2016} ~\cite{Satyanarayana2014}. 
In our data evaluation scheme will used the Bag of Little Bootstrap (BLB) ~\cite{ArXiv12066415}, which combines the results of bootstrapping multiple small subsets of a 
Big data dataset. THe BLB algorithm use an original Big dataset used generate small samples without replacements. 
For each generated sample another set of samples are created by resampling with replacement.

\subsection{Data Profiling}

Data profiling is an exploratory approach to data quality analysis. Statistical approaches are used to reveal 
data usage patterns as well as patterns in the data ~\cite{Osborne2013} ~\cite{Maydanchik2007}. Several tools exist for data quality assessment using data profiling 
and exploratory data analysis. Such tools include Tableau and Talend Open Studio. 

Data profiling module performs screening of data quality
based on statistics and information summary. Since
profiling is meant to discover data characteristics from data sources. It is considered as data assessment process that
provides a first summary of the data quality. Such information include: data format description, different
attributes, their types and values. data constraints (if any),
data range, max and min. More precisely information about
the data are presented in two types; technical and functional. This information can be extracted from the data itself
without any additional representation using it metadata or any descriptive header file, or by parsing the data using any
analysis tools. This task may become very costly in Big Data. To avoid costs generated due the data size we will use
the same sampling process BLB to reduce the data into a representative population sample, in addition to the combination of profiling results.
