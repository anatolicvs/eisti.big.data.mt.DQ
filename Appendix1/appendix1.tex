%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\chapter{How to use NettoyageML} 

\begin{flushleft}
	This is the EISTI.NettoyageML Benchmark for Joint Data Cleaning and Machine Learning. 
	The codebase is located in: \url{https://github.com/aytacozkan/EISTI.NettoyageML}
\end{flushleft}

\section{Basic Usage}

\subsection*{Run Experiments}

\begin{flushleft}
	To run experiments, download and unzip the 
	\href{https://drive.google.com/file/d/1BdLZAyp5KSvj8AdL2by9bNkRxX8po1FL/view?usp=sharing}{datasets}. 
	Place it under the project home directory and execute the following command from the project home directory:
\end{flushleft}


\begin{lstlisting}[language=python]
		python3 main.py --run_experiments [--dataset <name>] \
		\[--cpu <num_cpu>][--log]
\end{lstlisting}

\subsubsection*{Options}

\begin{itemize}
	\item {
		\textbf{--dataset:} the experiment dataset. If not specified, the program will run experiments on all datasets.
	}
	\item {
		\textbf{--cpu:} the number of cpu used for experiment. Default is 1.
	}
	\item {
	\textbf{--log:} whether to log experiment process.
	}
\end{itemize}


\subsubsection*{Output}

The experimental results for each dataset will be saved in `/result` directory as a json file named as <dataset name>\_result.json. Each result is a key-value pair. The key is a string in format "<dataset><split seed><error type><clean method><ML model><random search seed>". The value is a set of key-value pairs for each evaluation metric and result. Our experimental results are provided in `result.zip`.


\subsection*{Run Analysis}
To run analysis for populating relations described in the paper, unzip \textbf{`result.zip`} and execute the following command from the project home directory:

\begin{lstlisting}[language=python]
		python3 main.py --run_analysis [--alpha <value>]
\end{lstlisting}

\subsubsection*{Options}

\begin{itemize}
	\item {
	\textbf{--alpha:} the significance level for multiple hypothesis test. Default is 0.05.	
}
\end{itemize}


\subsubsection*{Output}
The relations R1, R2 and R3 will be saved in \textbf{`/analysis`} directory. Our analysis results are provided in `analysis.zip`.


\section{Extend Domain of Attributes}

\subsection*{Add new datasets}

To add a new dataset, first, create a new folder with dataset name under \textbf{`/data`} and create a `raw` folder under the new folder.  The `raw` folder must contain raw data named \textbf{`raw.csv`}. For dataset with inconsistencies, it must also contain the inconsistency-cleaned version data named \textbf{`inconsistency\_clean\_raw.csv`}. For dataset with mislabels, it must also contain the mislabel-cleaned version data named \textbf{`mislabel\_clean\_raw.csv`}. The structure of the directory looks like:


\dirtree{%
	.1 /.
	.2 data.
	.3 new\_dataset.
	.4 raw.
	.5 raw.csv.
	.5 inconsistency\_clean\_raw.csv (for dataset with inconsistencies).
	.5 mislabel\_clean\_raw.csv (for dataset with mislabels).
}

Then add a dictionary to \textbf{`/schema/dataset.py`} and append it to \textbf{`datasets`} array at the end of the file.

The new dictionary must contain the following keys:

\begin{lstlisting}[language=xml]
data_dir: the name of the dataset.
error_types: a list of error types that the dataset contains.
label: the label of ML task.
\end{lstlisting}

The following keys are optional:

\begin{lstlisting}[language=xml]
class_imbalance: whether the dataset is class imbalanced.
categorical_variables: a list of categorical attributes.
text_variables: a list of text attributes.
key_columns: a list of key columns used for deduplication.
drop_variables: a list of irrelevant attributes.
\end{lstlisting}

