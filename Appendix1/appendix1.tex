%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\chapter{How to use NettoyageML} 

\begin{flushleft}
	This is the EISTI.NettoyageML Benchmark for Joint Data Cleaning and Machine Learning. 
	The codebase is located in: \url{https://github.com/magnusanatolius/NettoyageML}
\end{flushleft}

\section{Basic Usage}

\subsection*{Run Experiments}

\begin{flushleft}
	To run experiments, download and unzip the 
	\href{https://drive.google.com/file/d/1BdLZAyp5KSvj8AdL2by9bNkRxX8po1FL/view?usp=sharing}{datasets}. 
	Place it under the project home directory and execute the following command from the project home directory:
\end{flushleft}


\begin{lstlisting}[language=python]
		python3 main.py --run_experiments [--dataset <name>] \
		\[--cpu <num_cpu>][--log]
\end{lstlisting}

\subsubsection*{Options}

\begin{itemize}
	\item {
		\textbf{--dataset:} the experiment dataset. If not specified, the program will run experiments on all datasets.
	}
	\item {
		\textbf{--cpu:} the number of cpu used for experiment. Default is 1.
	}
	\item {
	\textbf{--log:} whether to log experiment process.
	}
\end{itemize}


\subsubsection*{Output}

The experimental results for each dataset will be saved in `/result` directory as a json file named as <dataset name>\_result.json. Each result is a key-value pair. The key is a string in format "<dataset><split seed><error type><clean method><ML model><random search seed>". The value is a set of key-value pairs for each evaluation metric and result. Our experimental results are provided in `result.zip`.


\subsection*{Run Analysis}
To run analysis for populating relations described in the paper, unzip \textbf{`result.zip`} and execute the following command from the project home directory:

\begin{lstlisting}[language=python]
		python3 main.py --run_analysis [--alpha <value>]
\end{lstlisting}

\subsubsection*{Options}

\begin{itemize}
	\item {
	\textbf{--alpha:} the significance level for multiple hypothesis test. Default is 0.05.	
}
\end{itemize}


\subsubsection*{Output}
The relations R1, R2 and R3 will be saved in \textbf{`/analysis`} directory. Our analysis results are provided in `analysis.zip`.


\section{Extend Domain of Attributes}

\subsection*{Add new datasets}

To add a new dataset, first, create a new folder with dataset name under \textbf{`/data`} and create a `raw` folder under the new folder.  The `raw` folder must contain raw data named \textbf{`raw.csv`}. For dataset with inconsistencies, it must also contain the inconsistency-cleaned version data named \textbf{`inconsistency\_clean\_raw.csv`}. For dataset with mislabels, it must also contain the mislabel-cleaned version data named \textbf{`mislabel\_clean\_raw.csv`}. The structure of the directory looks like:


\dirtree{%
	.1 /.
	.2 data.
	.3 new\_dataset.
	.4 raw.
	.5 raw.csv.
	.5 inconsistency\_clean\_raw.csv (for dataset with inconsistencies).
	.5 mislabel\_clean\_raw.csv (for dataset with mislabels).
}

Then add a dictionary to \textbf{`/schema/dataset.py`} and append it to \textbf{`datasets`} array at the end of the file.

The new dictionary must contain the following keys:

\begin{lstlisting}[language=xml]
data_dir: the name of the dataset.
error_types: a list of error types that the dataset contains.
label: the label of ML task.
\end{lstlisting}

The following keys are optional:

\begin{lstlisting}[language=xml]
class_imbalance: whether the dataset is class imbalanced.
categorical_variables: a list of categorical attributes.
text_variables: a list of text attributes.
key_columns: a list of key columns used for deduplication.
drop_variables: a list of irrelevant attributes.
\end{lstlisting}

\subsection*{Add new error types}
To add a new error type, add a dictionary to `/schema/error\_type.py` and append it to `error\_types` array at the end of the file.

The new dictionary must contain the following keys:

\begin{lstlisting}[language=xml]
name: the name of the error type.
cleaning_methods: a dictionary, {cleaning method name: cleaning methods object}.
\end{lstlisting}

\subsection*{Add new models}
To add a new ML model, add a dictionary to `/schema/model.py` and append it to `models` array at the end of the file.

The new dictionary must contain the following keys:

\begin{lstlisting}[language=xml]
name: the name of the model.
fn: the function of the model.
fixed_params: parameters not to be tuned.
hyperparams: the hyperparameter to be tuned.
hyperparams_type: the type of hyperparameter "real" or "int".
hyperparams_range: range of search. Use log base for real type hyperparameters.
\end{lstlisting}


\subsection*{Add new cleaning methods}
\begin{flushleft}
	To add a new cleaning methods, add a class to `/schema/cleaning\_method.py`.

The class must contain two methods:
\end{flushleft}

\textbf{`fit(dataset, dirty\_train)`:} take in the dataset dictionary and dirty training set. Compute statistics or train models on training set for data cleaning.

\textbf{`clean(dirty\_train, dirty\_test)`}: take in the dirty training set and dirty test set. Clean the error in the training set and test set. Return \textbf{`(clean\_train, indicator\_train, clean\_test, indicator\_test)`}, which are the clean version datasets and indicators that indicate the location of error. 

\subsection*{Add new scenarios}
We consider "BD" and "CD" scenarios in our paper. To investigate other scenarios, add scenarios to `/schema/scenario.py`.

