%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Forth Chapter **********************************
%*******************************************************************************
\chapter{Experimentations , Results and Analysis}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

In this chapter, we will explain how data quality dimensions are implemented with Machine Learning (ML) techniques towards dirty datasets. 

\section{Experimental Methodology}  

The impact of the dirt data and data cleaning on ML in a dataset depends on a number of factors -- some factors depend on the data cleaning process, 

i.e., the error types to be cleaned and the cleaning methods; some factors depend on the ML
process, i.e., the model types used; and some factors depend on where the cleaning is performed during the ML process. Hence, in order to comprehensively investigate the impacts, we need to consider data cleaning an ML jointly in our experiments.


\section{The NettoyageML ~\cite{Nettoyage2019} Schema}  

The NettoyageML relational schema consists of three relations as shown in Table ~\ref{table:nettoyage_ml} . Firstly will introduced the attributes of NettoyageML relational models, and then will explained 
the differences between these three relations.


\begin{itemize}
	\item {
		\textbf{Attributes for Dataset.} The first attribute is dataset, which is the input to the data cleaning and ML pipeline. Each dataset can have multiply types of errors and has an associated ML task. 
	}
\end{itemize}


\begin{table}[H]	
	\leftskip=3em
	\begin{flushleft}
		\leftskip=3em
		\textbf{R1 Vanilla}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & ML Model & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak	

	\begin{flushleft}
		\leftskip=3em
		\textbf{R2 (With Model Section)}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak

	\begin{flushleft}
		\leftskip=3em
		\textbf{R3 (With Model Selection and Cleaning Method Selection)}
	\end{flushleft}	

	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak
	\caption{NettoyageML Relational Schema}
	\label{table:nettoyage_ml}
\end{table}

Even for one error type, they might appear in a dataset in various distributions and hence affect ML models in complicated ways/ To study the impact of the real-world error types and distribution on ML models, we mostly use real-world-dataset with real errors, and we apply various cleaning methods to detect and repair the errors in them.


\begin{itemize}
	\item {
	\textbf{Attributes for Data Cleaning.}	
		The error type attribute describes
		which type of dirtiness we are testing. We consider five most
		common types of dirtiness that are considered in the ML and
		DB communities: missing values, outliers, duplicates, inconsistencies, and mislabels. For each error type, there exist multiple
		cleaning methods, and each cleaning method includes an error
		detection component and an error repair component.
	}
\item {
	\textbf{Attributes for ML.}	
	The ML model describes the algorithm of
	training and prediction. Different ML models may have different
	robustness or sensitivity to dirty data.
}
\item {
	\textbf{Flag Attribute.}	
	\textit{The flag attribute summarizes } the impact of data cleaning on ML of an experiment into three categories “\textbf{P (positive)}”, “\textbf{N (negative)}” 
	or “\textbf{S (insignificant)}”, indicating whether
	the cleaning has a positive, negative, or insignificant impact on
	the ML performance respectively.
}
\end{itemize}

Given these attributes, we designed three relations as shown Table ~\ref{table:nettoyage_ml}
\textit{R1} shows the vanilla of the a NettoyageML relation schema with the key 
\textit{ \{\ dataset, error type, detect, repair, ML model, scenario, flag \}\ }
Every tuple of \textit{R1} represents \underline{a hypothesis} or \underline{an experiment:}
how does cleaning some type of error using a detection method and a repair method affect a ML model for a given dataset?
We also consider other two versions of relations in NettoyageML. 

Compared with \textit{R1}, \textit{R2} eliminates the ML model attribute. In this case,
we try different models during training and select the model that
has the best validation accuracy (or F1 score) as the model to be
considered in an experiment in \textit{R2}. 

Every tuple of R2 represents a hypothesis or an experiment: how does cleaning some type of error using a detection method and a repair method affect the best
ML model for a given dataset? 

\textit{R3} further eliminates the cleaning
method (detection and repair) attributes. In this case, in addition
to model selection, we also try different cleaning methods and select the cleaning method that results in the best validation accuracy
as the cleaning method to be considered in an experiment in \textit{R3}.
Every tuple of \textit{R3} represents a hypothesis or an experiment: how
does the best cleaning method affect the predictive performance of
the best model for a given dataset?

All three relations can also be extended with other attributes that are associated with an experiment, which may help obtain insights
and interpret the results, such as the accuracy scores before and
after cleaning and p-values of hypothesis testing associated with
each experiment.


\section{Analysis Methodology}

are associated with an experiment, which may help obtain insights
and interpret the results, such as the accuracy scores before and
after cleaning and p-values of hypothesis testing associated with
each experiment in Table  ~\ref{table:nettoyage_ml} . 

We first fix the error type and group by flags. The percentage of each type of flag indicates the general impact of cleaning this type of error on ML. For example,
if flag “P” dominates in the error type outliers, it indicates cleaning outliers generally improves the performance of ML. Then we
group by the other attributes (e.g., ML models, datasets, etc.) in addition to flags to see how each attribute affects the impact. 

For example, if flag “S” dominates under the ML algorithm decision tree, it indicates decision tree is insensitive to this error.
We also investigate the changes of percentage in each type of flag when moving from \textit{R1}, to \textit{R2} and \textit{R3}. 

This indicates how model selection and cleaning algorithm selection affect the impacts. 

For example, if the percentage of flag “N” significantly decreases from
\textit{R1} to \textit{R2}, it indicates the model selection reduces the negative impact of data cleaning on ML.

We investigate the impact of dirty data on ML models by simply running SQL queries on three relations \textit{R1}, \textit{R2} and \textit{R3}. 

We formally present the SQL query templates as follows, where E $\in$ {inconsistencies, duplicates, mislabels, outliers, missing values}.

\textbf{Q1: Flag}
\begin{lstlisting}[language=SQL, caption=Q1: Flag]
	SELECT flag, COUNT(*) FROM R 
	WHERE error_type = E 
	GROUP BY flag
\end{lstlisting}

\textbf{Q2: Scenario}
\begin{lstlisting}[language=SQL, caption=Q2: Scenario]
	SELECT scenario, flag, COUNT(*)
	FROM R
	WHERE error_type = E
  GROUP BY scenario, flag
\end{lstlisting}

\textbf{Q3: ML Model. (Not applicable to R2, R3)}
\begin{lstlisting}[language=SQL, caption=Q3: ML Model]
	SELECT model, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY model, flag
\end{lstlisting}

\textbf{Q4: Clean Method (Not applicable to R3 or E $\in$ { inconsistencies, duplicates, mislabels}, where only one cleaning method is applied}
\begin{lstlisting}[language=SQL, caption=Q4.1:Clean Method]
	SELECT detect_method, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY detect_method, flag
\end{lstlisting}

\begin{lstlisting}[language=SQL, caption=Q4.2:Clean Method]
	SELECT repair_method, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY repair_method, flag
\end{lstlisting}
-\\
\begin{lstlisting}[language=SQL, caption=Q5: Dataset]
	SELECT dataset, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY dataset, flag
\end{lstlisting}

\section{Design of Benchmark}

Based on NettoyageML Relational Schema, we design NettoyageML Benchmark by specifying the domain of each key attribute.In the following sections we present all datasets, error types, cleaning methods, ML models, and cleaning scenarios we consider in the benchmark.

\section{Error Types and Cleaning Methods}
We consider five error types that are prevalent in the real-world
datasets, including missing values, outliers, duplicates, inconsistencies and mislabels. 

\begin{longtable}[c]{|l|l|l|}
	\caption{Cleaning Methods}
	\label{tab:cleaning-methods-table}\\
	\hline
	\textbf{Error Type}       & \textbf{Detect Method} & \textbf{Repair Method}                                                                                                             \\ \hline
	\endfirsthead
	%
	\endhead
	%
	Missing Values            & Empty Entry            & Deletion                                                                                                                           \\ \hline
	&                        & \begin{tabular}[c]{@{}l@{}}Imputation:\\ Mean Mode\\ Median Mode\\ Mode Mode\\ Mean Dummy\\ Mode Dummy\\ Median Dummy\end{tabular} \\ \hline
	\multirow{3}{*}{Outliers} & SD                     & Deletion                                                                                                                           \\ \cline{2-3} 
	& IQR                    & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Imputation:\\ Mean\\ Median\\ Mode\end{tabular}}                                        \\ \cline{2-2}
	& IF                     &                                                                                                                                    \\ \cline{1-2}
	\multicolumn{2}{|l|}{\multirow{2}{*}{}}            &                                                                                                                                    \\
	\multicolumn{2}{|l|}{}                             &                                                                                                                                    \\ \hline
	Duplicates                & Key Collision          & Deletion                                                                                                                           \\ \hline
	Inconsistencies           & OpenRefine             & Merge                                                                                                                              \\ \hline
	Mislabels                 & Ground Truth           & Flip Labels                                                                                                                        \\ \hline
\end{longtable}
While there are many cleaning methods in the literature we consider the most straightforward
one in this study. 
The definition and the cleaning methods of each
error type are described below and summarized in Table ~\ref{tab:cleaning-methods-table}

\subsection{Missing Values}
Missing values occur when no value is stored for some attribute
in an observation. We detect missing values by finding empty or
\textit{NaN} entries in datasets. We use two methods to repair missing
values:
\begin{itemize}
	\item{\textbf{Deletion:} Delete records with missing values.
	}
	\item{
		\textbf{Imputation:} For numerical missing values, we consider three
		types of imputation methods: mean, median and mode. For categorical missing values, we use two types of imputation methods: the mode (most frequent class) or a dummy variable named
		“missing”. Therefore, we have six imputation methods. In Table ~\ref{tab:cleaning-methods-table} we denote each imputation method by the numerical imputation and categorical imputation (e.g. “Mean Dummy” represents
		imputing numerical missing values by mean and imputing categorical missing values by dummy variables).
	}
\end{itemize}

\subsection{Outliers}
An outlier is an observation that is distant from other observations. We only consider numerical outliers in our experiments. We use three methods to detect numerical outliers.

\begin{itemize}
	\item {\textbf{Standard Deviation Method (SD:)}
		A value is considered to be an outlier if it is $n$ numbers of standard deviations away from the mean of the attribute it belongs to. We use $ n = 3 $.
	}
	\item {\textbf{Interquartile Range Method (IQR):} Let $Q_{1}$ and $Q_{3}$ be the 25\textit{th} and the 75\textit{th} percentiles of an attribute. Then, interquartile range 
		$ IQR = Q_{3} - Q_{1} $. A value is considered to be an outlier of it is outside range
		of $ [Q_{1} - k * IQR, Q_{3} + k * IQR] $ We use $ k = 1.5$.
	}
	\item{\textbf{Isolation Forest Method (IF):}
		The isolation forest isolates observations by randomly selecting a feature and randomly selecting a
		split value of the selected feature. This partition can be represented by a tree structure and outliers will have noticeably shorter
		paths in the random trees. We use \textbf{the scikit-learn IsolationForest}
		and set the contamination parameter to be $\mathbf{0.01}$
	}
\end{itemize}
We use two methods to repair numerical outliers in the datasets.
\begin{itemize}
	\item {
		\textbf{Deletion:} Records with outliers entries are removed from the datasets.
	}
	\item {
		\textbf{Imputation:}Outliers entries are imputed. We consider three
		types of imputation:  \textbf{mean}, \textbf{median} and \textbf{mode}.
	}
\end{itemize}

\subsection{Duplicates}
Duplicates refer to the records that correspond to the identical
real-world entity. We detect duplicates by defining the key attribute
that is unique for each entity in the dataset. If two records have an
identical value on the key attribute, they will be considered as duplicates. We repair the duplicates by keeping the first and deleting all the others.

\subsection{Inconsistencies}
Inconsistencies occur when two values correspond to the identical real-world entity but have different representations. 
We detect inconsistencies in datasets using OpenRefine ~\cite{Verborgh2013} and repair them by merging different representations into one in OpenRefine.

\subsection{Mislabels}
Mislabels occur when an observation is incorrectly labeled. Since
mislabels in our datasets come from error injection, we already
know which label is incorrect. We repair mislabels by flipping the
label. Our protocol in injecting class noise follows the recommendation in ~\cite{Garcia2015}

\section{Datasets}
Collected 13 real-world datasets from different sources to
conduct our experiments. Each dataset contains one or more types
of error summarized in Table ~\ref{tab:dataset-and-error-types}  For mislabels, we cannot find existing real-world datasets with both mislabels and ground truth. Since it is difficult to clean mislabels without ground truth (we do not know whether a label is mislabeled unless we have ground truth or domain knowledge), we injected mislabels in three real-world datasets.

-\\
\linebreak

\begin{longtable}[c]{|c|c|c|c|c|c|}
	\caption{Dataset and Error Types}
	\label{tab:dataset-and-error-types}\\
	\hline
	& \multicolumn{5}{c|}{\textbf{Error Types}} \\ \hline
	\endfirsthead
	%
	\endhead
	%
	\textbf{Datasets} & \textbf{Inconsistencies} & \textbf{Duplicates} & \textbf{Missing Values} & \textbf{Outliers} & \textbf{*Mislabels Data} \\ \hline
	Airbnb &  & X & X & X &  \\ \hline
	Citation &  & X &  &  &  \\ \hline
	Company & X &  &  &  &  \\ \hline
	Credit &  &  & X & X &  \\ \hline
	EEG &  &  &  & X & X \\ \hline
	KDD &  &  & X & X & X \\ \hline
	Marketing &  &  & X &  &  \\ \hline
	Movie & X & X &  &  &  \\ \hline
	Restaurant & X & X &  &  &  \\ \hline
	Sensor &  &  &  & X &  \\ \hline
	Titanic &  &  & X &  &  \\ \hline
	University & X &  &  &  &  \\ \hline
	USCensus &  &  & X &  & X \\ \hline
\end{longtable}


\begin{itemize}
	\item {
	\textbf{Airbnb :} This dataset contains 42,492 records on hotels in the top
	10 tourist destinations and major US metropolitan areas scraped
	from Airbnb.com. Each record has 40 attributes including the number of bedrooms, price, location, etc. Demographic and economic attributes were scraped from city-data.com. The classification task is to determine whether the rating of each hotel is 5 or not. This
	dataset contains missing values, numerical outliers and duplicates.	
}

\item {
	\textbf{Citation: ~\cite{magellandata}} This dataset consists of titles of 5,005 publications from Google Scholar and DBLP. Given a publication title, the classification task is to determine whether the paper is related to Computer Science or not. This dataset contains duplicates.	
}

\item {
	\textbf{Company: ~\cite{Hatton2019}} The original dataset contains over 2.5 million records
	about companies including company names and locations. Because of its large size, we randomly sampled 5% records (128,889
	records) from the original dataset. Each record has seven attributes including company name, country, city, etc. The classification task
	is to predict whether the company sentiment is negative or not. This dataset contains inconsistent company names.
}

\item {
	\textbf{Credit: ~\cite{creditdataset2019}}
	This dataset consists of 150,000 credit data with 10
	attributes including monthly income, age, then number of dependents, etc. The classification task for this dataset is to predict whether
	a client will experience financial distress in the next two years. This
	dataset has a class imbalance problem. There are only 6.7% records
	in minority class. This dataset primarily contains missing values
	and numerical outliers.
}

\item {
	\textbf{EEG: ~\cite{Dua:2019}}
	This is a dataset of 14,980 EEG recordings. Each record
	has 14 EEG attributes. The classification task is to predict whether
	the eye-state is closed or open. This dataset contains numerical outliers. We inject mislabels into this dataset by randomly flip labels.
}

\item {
	\textbf{KDD: ~\cite{KDDdataset2019}}
	his dataset contains 131,329 records about projects and
	donations from DonorsChoose.org. Each record has 100 attributes. The classification task is to predict whether a project is “exciting”.
	This dataset has a class imbalance problem. There are 11\% records
	in the minority class. This dataset contains missing values and numerical outliers. We inject mislabels into this dataset by randomly
	flip labels.
}

\item {
	\textbf{Marketing:}
	This dataset consists of 8,993 data about household in-
	come from a survey. Each record has 14 demographic attributes including sex, age, education, etc. The classification task is to predict
	if the annual household income is less than \$25000. This dataset contains missing values.
}

\item {
	\textbf{Movie: ~\cite{TMDBmovie2019} ~\cite{IMDBmovie2019}}
	This dataset consists of 9,329 movie reviews, which
	we obtained by merging data from IMDB and TMDB datasets.
	Each record has seven attributes including title, language, score,
	etc. The classification task is to predict the genre of the movie
	(romance or comedy). 
	It contains duplicates and inconsistent representations of languages.
}

\item{
	\textbf{Restaurant: ~\cite{RestaurantDataset2019}}
	This dataset contains 12,007 records about restaurants, which we obtained by merging data from the Yelp and Yellowpages datasets. Each record has 10 attributes including city, category, rating, etc. The classification task is to predict whether the
	categorical price range of a restaurant is “\$” or not. This dataset
	consists of duplicates and inconsistent restaurant names and categories.
}

\item {
	\textbf{Titanic: ~\cite{Titanic2019}}
	This dataset contains 891 records and 11 attributes from the Titanic including name, sex, age, etc. The classification
	task is to determine whether the passenger survived or not. This dataset has a significant number of missing values.
}

\item {
	\textbf{Sensor: ~\cite{SensorDataset2019}}
	The original sensor dataset contains 928,991 sensor recordings with eight attributes including temperature, humidity, light, etc. Because of the large size, we only used recordings from
sensor 1 and sensor 2 and sampled the dataset to include 1 observation per hour, and day for each 	sensor. The sampled dataset contains 62,076 records. The classification task is to predict whether
	the readings came from a particular sensor (sensor 1 or sensor 2). This dataset contains outliers.
}

\item {
	\textbf{University: ~\cite{University:2019}}
	This dataset contains 286 records about universities. Each record has 17 attributes including state, university name,
	SAT scores, etc. The classification task is to predict whether the
	expenses are greater than 7, 000 for each university. This dataset
	contains inconsistent representations for states and locations.
}

\item  {
	\textbf{USCensus: ~\cite{USCensusData:2019}}
	This dataset contains 32,561 US Census records
	for adults. Each record has 14 attributes including age, education,
	sex, etc. The classification goal is to predict whether the adult earns
	more than \$50,000. This dataset contains missing values.
}

\end{itemize}

\section{ML Models}

We select seven classical and competitive classification algorithms in our experiments, including \textbf{Logistic Regression, KNN,Decision Tree, Random Forest, Adaboost, XGBoost and Naive Bayes}.
We used \textbf{scikit-learn} ~\cite{Scikit-learn:2011} to train models. Each model is described
below.






