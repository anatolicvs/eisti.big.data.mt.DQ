%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Forth Chapter **********************************
%*******************************************************************************
\chapter{Experimentations , Results and Analysis}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

In this chapter, we will explain how data quality dimensions are implemented with Machine Learning (ML) techniques towards dirty datasets. 

\section{Experimental Methodology}  

The impact of the dirt data and data cleaning on ML in a dataset depends on a number of factors -- some factors depend on the data cleaning process, 

i.e., the error types to be cleaned and the cleaning methods; some factors depend on the ML
process, i.e., the model types used; and some factors depend on where the cleaning is performed during the ML process. Hence, in order to comprehensively investigate the impacts, we need to consider data cleaning an ML jointly in our experiments.


\section{The NettoyageML ~\cite{Nettoyage2019} Schema}  

The NettoyageML relational schema consists of three relations as shown in Table ~\ref{table:nettoyage_ml} . Firstly will introduced the attributes of NettoyageML relational models, and then will explained 
the differences between these three relations.


\begin{itemize}
	\item {
		\textbf{Attributes for Dataset.} The first attribute is dataset, which is the input to the data cleaning and ML pipeline. Each dataset can have multiply types of errors and has an associated ML task. 
	}
\end{itemize}


\begin{table}[H]	
	\leftskip=3em
	\begin{flushleft}
		\leftskip=3em
		\textbf{R1 Vanilla}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & ML Model & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak	

	\begin{flushleft}
		\leftskip=3em
		\textbf{R2 (With Model Section)}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak

	\begin{flushleft}
		\leftskip=3em
		\textbf{R3 (With Model Selection and Cleaning Method Selection)}
	\end{flushleft}	

	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Scenario & Flag \\ 
		\hline 
	\end{tabular} \linebreak
	\caption{NettoyageML Relational Schema}
	\label{table:nettoyage_ml}
\end{table}

Even for one error type, they might appear in a dataset in various distributions and hence affect ML models in complicated ways/ To study the impact of the real-world error types and distribution on ML models, we mostly use real-world-dataset with real errors, and we apply various cleaning methods to detect and repair the errors in them.


\begin{itemize}
	\item {
	\textbf{Attributes for Data Cleaning.}	
		The error type attribute describes
		which type of dirtiness we are testing. We consider five most
		common types of dirtiness that are considered in the ML and
		DB communities: missing values, outliers, duplicates, inconsistencies, and mislabels. For each error type, there exist multiple
		cleaning methods, and each cleaning method includes an error
		detection component and an error repair component.
	}
\item {
	\textbf{Attributes for ML.}	
	The ML model describes the algorithm of
	training and prediction. Different ML models may have different
	robustness or sensitivity to dirty data.
}
\item {
	\textbf{Flag Attribute.}	
	\textit{The flag attribute summarizes } the impact of data cleaning on ML of an experiment into three categories “\textbf{P (positive)}”, “\textbf{N (negative)}” 
	or “\textbf{S (insignificant)}”, indicating whether
	the cleaning has a positive, negative, or insignificant impact on
	the ML performance respectively.
}
\end{itemize}

Given these attributes, we designed three relations as shown Table ~\ref{table:nettoyage_ml}
\textit{R1} shows the vanilla of the a NettoyageML relation schema with the key 
\textit{ \{\ dataset, error type, detect, repair, ML model, scenario, flag \}\ }
Every tuple of \textit{R1} represents \underline{a hypothesis} or \underline{an experiment:}
how does cleaning some type of error using a detection method and a repair method affect a ML model for a given dataset?
We also consider other two versions of relations in NettoyageML. 

Compared with \textit{R1}, \textit{R2} eliminates the ML model attribute. In this case,
we try different models during training and select the model that
has the best validation accuracy (or F1 score) as the model to be
considered in an experiment in \textit{R2}. 

Every tuple of R2 represents a hypothesis or an experiment: how does cleaning some type of error using a detection method and a repair method affect the best
ML model for a given dataset? 

\textit{R3} further eliminates the cleaning
method (detection and repair) attributes. In this case, in addition
to model selection, we also try different cleaning methods and select the cleaning method that results in the best validation accuracy
as the cleaning method to be considered in an experiment in \textit{R3}.
Every tuple of \textit{R3} represents a hypothesis or an experiment: how
does the best cleaning method affect the predictive performance of
the best model for a given dataset?

All three relations can also be extended with other attributes that are associated with an experiment, which may help obtain insights
and interpret the results, such as the accuracy scores before and
after cleaning and p-values of hypothesis testing associated with
each experiment.


\section{Analysis Methodology}\label{sec:analysis-methodology}

are associated with an experiment, which may help obtain insights
and interpret the results, such as the accuracy scores before and
after cleaning and p-values of hypothesis testing associated with
each experiment in Table  ~\ref{table:nettoyage_ml} . 

We first fix the error type and group by flags. The percentage of each type of flag indicates the general impact of cleaning this type of error on ML. For example,
if flag “P” dominates in the error type outliers, it indicates cleaning outliers generally improves the performance of ML. Then we
group by the other attributes (e.g., ML models, datasets, etc.) in addition to flags to see how each attribute affects the impact. 

For example, if flag “S” dominates under the ML algorithm decision tree, it indicates decision tree is insensitive to this error.
We also investigate the changes of percentage in each type of flag when moving from \textit{R1}, to \textit{R2} and \textit{R3}. 

This indicates how model selection and cleaning algorithm selection affect the impacts. 

For example, if the percentage of flag “N” significantly decreases from
\textit{R1} to \textit{R2}, it indicates the model selection reduces the negative impact of data cleaning on ML.

We investigate the impact of dirty data on ML models by simply running SQL queries on three relations \textit{R1}, \textit{R2} and \textit{R3}. 

We formally present the SQL query templates as follows, where E $\in$ {inconsistencies, duplicates, mislabels, outliers, missing values}.

\textbf{Q1: Flag}
\begin{lstlisting}[language=SQL, caption=Q1: Flag]
	SELECT flag, COUNT(*) FROM R 
	WHERE error_type = E 
	GROUP BY flag
\end{lstlisting}

\textbf{Q2: Scenario}
\begin{lstlisting}[language=SQL, caption=Q2: Scenario]
	SELECT scenario, flag, COUNT(*)
	FROM R
	WHERE error_type = E
  GROUP BY scenario, flag
\end{lstlisting}

\textbf{Q3: ML Model. (Not applicable to R2, R3)}
\begin{lstlisting}[language=SQL, caption=Q3: ML Model]
	SELECT model, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY model, flag
\end{lstlisting}

\textbf{Q4: Clean Method (Not applicable to R3 or E $\in$ { inconsistencies, duplicates, mislabels}, where only one cleaning method is applied}
\begin{lstlisting}[language=SQL, caption=Q4.1:Clean Method]
	SELECT detect_method, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY detect_method, flag
\end{lstlisting}

\begin{lstlisting}[language=SQL, caption=Q4.2:Clean Method]
	SELECT repair_method, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY repair_method, flag
\end{lstlisting}
-\\
\begin{lstlisting}[language=SQL, caption=Q5: Dataset]
	SELECT dataset, flag, COUNT(*)
	FROM R
	WHERE error_type = E
	GROUP BY dataset, flag
\end{lstlisting}

\section{Design of Benchmark}

Based on NettoyageML Relational Schema, we design NettoyageML Benchmark by specifying the domain of each key attribute.In the following sections we present all datasets, error types, cleaning methods, ML models, and cleaning scenarios we consider in the benchmark.

\section{Error Types and Cleaning Methods}
We consider five error types that are prevalent in the real-world
datasets, including missing values, outliers, duplicates, inconsistencies and mislabels. 

\begin{longtable}[c]{|l|l|l|}
	\caption{Cleaning Methods}
	\label{tab:cleaning-methods-table}\\
	\hline
	\textbf{Error Type}       & \textbf{Detect Method} & \textbf{Repair Method}                                                                                                             \\ \hline
	\endfirsthead
	%
	\endhead
	%
	Missing Values            & Empty Entry            & Deletion                                                                                                                           \\ \hline
	&                        & \begin{tabular}[c]{@{}l@{}}Imputation:\\ Mean Mode\\ Median Mode\\ Mode Mode\\ Mean Dummy\\ Mode Dummy\\ Median Dummy\end{tabular} \\ \hline
	\multirow{3}{*}{Outliers} & SD                     & Deletion                                                                                                                           \\ \cline{2-3} 
	& IQR                    & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Imputation:\\ Mean\\ Median\\ Mode\end{tabular}}                                        \\ \cline{2-2}
	& IF                     &                                                                                                                                    \\ \cline{1-2}
	\multicolumn{2}{|l|}{\multirow{2}{*}{}}            &                                                                                                                                    \\
	\multicolumn{2}{|l|}{}                             &                                                                                                                                    \\ \hline
	Duplicates                & Key Collision          & Deletion                                                                                                                           \\ \hline
	Inconsistencies           & OpenRefine             & Merge                                                                                                                              \\ \hline
	Mislabels                 & Ground Truth           & Flip Labels                                                                                                                        \\ \hline
\end{longtable}
While there are many cleaning methods in the literature we consider the most straightforward
one in this study. 
The definition and the cleaning methods of each
error type are described below and summarized in Table ~\ref{tab:cleaning-methods-table}

\subsection{Missing Values}
Missing values occur when no value is stored for some attribute
in an observation. We detect missing values by finding empty or
\textit{NaN} entries in datasets. We use two methods to repair missing
values:
\begin{itemize}
	\item{\textbf{Deletion:} Delete records with missing values.
	}
	\item{
		\textbf{Imputation:} For numerical missing values, we consider three
		types of imputation methods: mean, median and mode. For categorical missing values, we use two types of imputation methods: the mode (most frequent class) or a dummy variable named
		“missing”. Therefore, we have six imputation methods. In Table ~\ref{tab:cleaning-methods-table} we denote each imputation method by the numerical imputation and categorical imputation (e.g. “Mean Dummy” represents
		imputing numerical missing values by mean and imputing categorical missing values by dummy variables).
	}
\end{itemize}

\subsection{Outliers}
An outlier is an observation that is distant from other observations. We only consider numerical outliers in our experiments. We use three methods to detect numerical outliers.

\begin{itemize}
	\item {\textbf{Standard Deviation Method (SD:)}
		A value is considered to be an outlier if it is $n$ numbers of standard deviations away from the mean of the attribute it belongs to. We use $ n = 3 $.
	}
	\item {\textbf{Interquartile Range Method (IQR):} Let $Q_{1}$ and $Q_{3}$ be the 25\textit{th} and the 75\textit{th} percentiles of an attribute. Then, interquartile range 
		$ IQR = Q_{3} - Q_{1} $. A value is considered to be an outlier of it is outside range
		of $ [Q_{1} - k * IQR, Q_{3} + k * IQR] $ We use $ k = 1.5$.
	}
	\item{\textbf{Isolation Forest Method (IF):}
		The isolation forest isolates observations by randomly selecting a feature and randomly selecting a
		split value of the selected feature. This partition can be represented by a tree structure and outliers will have noticeably shorter
		paths in the random trees. We use \textbf{the scikit-learn IsolationForest}
		and set the contamination parameter to be $\mathbf{0.01}$
	}
\end{itemize}
We use two methods to repair numerical outliers in the datasets.
\begin{itemize}
	\item {
		\textbf{Deletion:} Records with outliers entries are removed from the datasets.
	}
	\item {
		\textbf{Imputation:}Outliers entries are imputed. We consider three
		types of imputation:  \textbf{mean}, \textbf{median} and \textbf{mode}.
	}
\end{itemize}

\subsection{Duplicates}
Duplicates refer to the records that correspond to the identical
real-world entity. We detect duplicates by defining the key attribute
that is unique for each entity in the dataset. If two records have an
identical value on the key attribute, they will be considered as duplicates. We repair the duplicates by keeping the first and deleting all the others.

\subsection{Inconsistencies}
Inconsistencies occur when two values correspond to the identical real-world entity but have different representations. 
We detect inconsistencies in datasets using OpenRefine ~\cite{Verborgh2013} and repair them by merging different representations into one in OpenRefine.

\subsection{Mislabels}
Mislabels occur when an observation is incorrectly labeled. Since
mislabels in our datasets come from error injection, we already
know which label is incorrect. We repair mislabels by flipping the
label. Our protocol in injecting class noise follows the recommendation in ~\cite{Garcia2015}

\section{Datasets}
Collected 13 real-world datasets from different sources to
conduct our experiments. Each dataset contains one or more types
of error summarized in Table ~\ref{tab:dataset-and-error-types}  For mislabels, we cannot find existing real-world datasets with both mislabels and ground truth. Since it is difficult to clean mislabels without ground truth (we do not know whether a label is mislabeled unless we have ground truth or domain knowledge), we injected mislabels in three real-world datasets.

-\\
\linebreak

\begin{longtable}[c]{|c|c|c|c|c|c|}
	\caption{Dataset and Error Types}
	\label{tab:dataset-and-error-types}\\
	\hline
	& \multicolumn{5}{c|}{\textbf{Error Types}} \\ \hline
	\endfirsthead
	%
	\endhead
	%
	\textbf{Datasets} & \textbf{Inconsistencies} & \textbf{Duplicates} & \textbf{Missing Values} & \textbf{Outliers} & \textbf{*Mislabels Data} \\ \hline
	Airbnb &  & X & X & X &  \\ \hline
	Citation &  & X &  &  &  \\ \hline
	Company & X &  &  &  &  \\ \hline
	Credit &  &  & X & X &  \\ \hline
	EEG &  &  &  & X & X \\ \hline
	KDD &  &  & X & X & X \\ \hline
	Marketing &  &  & X &  &  \\ \hline
	Movie & X & X &  &  &  \\ \hline
	Restaurant & X & X &  &  &  \\ \hline
	Sensor &  &  &  & X &  \\ \hline
	Titanic &  &  & X &  &  \\ \hline
	University & X &  &  &  &  \\ \hline
	USCensus &  &  & X &  & X \\ \hline
\end{longtable}


\begin{itemize}
	\item {
	\textbf{Airbnb :} This dataset contains 42,492 records on hotels in the top
	10 tourist destinations and major US metropolitan areas scraped
	from Airbnb.com. Each record has 40 attributes including the number of bedrooms, price, location, etc. Demographic and economic attributes were scraped from city-data.com. The classification task is to determine whether the rating of each hotel is 5 or not. This
	dataset contains missing values, numerical outliers and duplicates.	
}

\item {
	\textbf{Citation: ~\cite{magellandata}} This dataset consists of titles of 5,005 publications from Google Scholar and DBLP. Given a publication title, the classification task is to determine whether the paper is related to Computer Science or not. This dataset contains duplicates.	
}

\item {
	\textbf{Company: ~\cite{Hatton2019}} The original dataset contains over 2.5 million records
	about companies including company names and locations. Because of its large size, we randomly sampled 5% records (128,889
	records) from the original dataset. Each record has seven attributes including company name, country, city, etc. The classification task
	is to predict whether the company sentiment is negative or not. This dataset contains inconsistent company names.
}

\item {
	\textbf{Credit: ~\cite{creditdataset2019}}
	This dataset consists of 150,000 credit data with 10
	attributes including monthly income, age, then number of dependents, etc. The classification task for this dataset is to predict whether
	a client will experience financial distress in the next two years. This
	dataset has a class imbalance problem. There are only 6.7% records
	in minority class. This dataset primarily contains missing values
	and numerical outliers.
}

\item {
	\textbf{EEG: ~\cite{Dua:2019}}
	This is a dataset of 14,980 EEG recordings. Each record
	has 14 EEG attributes. The classification task is to predict whether
	the eye-state is closed or open. This dataset contains numerical outliers. We inject mislabels into this dataset by randomly flip labels.
}

\item {
	\textbf{KDD: ~\cite{KDDdataset2019}}
	his dataset contains 131,329 records about projects and
	donations from DonorsChoose.org. Each record has 100 attributes. The classification task is to predict whether a project is “exciting”.
	This dataset has a class imbalance problem. There are 11\% records
	in the minority class. This dataset contains missing values and numerical outliers. We inject mislabels into this dataset by randomly
	flip labels.
}

\item {
	\textbf{Marketing:}
	This dataset consists of 8,993 data about household in-
	come from a survey. Each record has 14 demographic attributes including sex, age, education, etc. The classification task is to predict
	if the annual household income is less than \$25000. This dataset contains missing values.
}

\item {
	\textbf{Movie: ~\cite{TMDBmovie2019} ~\cite{IMDBmovie2019}}
	This dataset consists of 9,329 movie reviews, which
	we obtained by merging data from IMDB and TMDB datasets.
	Each record has seven attributes including title, language, score,
	etc. The classification task is to predict the genre of the movie
	(romance or comedy). 
	It contains duplicates and inconsistent representations of languages.
}

\item{
	\textbf{Restaurant: ~\cite{RestaurantDataset2019}}
	This dataset contains 12,007 records about restaurants, which we obtained by merging data from the Yelp and Yellowpages datasets. Each record has 10 attributes including city, category, rating, etc. The classification task is to predict whether the
	categorical price range of a restaurant is “\$” or not. This dataset
	consists of duplicates and inconsistent restaurant names and categories.
}

\item {
	\textbf{Titanic: ~\cite{Titanic2019}}
	This dataset contains 891 records and 11 attributes from the Titanic including name, sex, age, etc. The classification
	task is to determine whether the passenger survived or not. This dataset has a significant number of missing values.
}

\item {
	\textbf{Sensor: ~\cite{SensorDataset2019}}
	The original sensor dataset contains 928,991 sensor recordings with eight attributes including temperature, humidity, light, etc. Because of the large size, we only used recordings from
sensor 1 and sensor 2 and sampled the dataset to include 1 observation per hour, and day for each 	sensor. The sampled dataset contains 62,076 records. The classification task is to predict whether
	the readings came from a particular sensor (sensor 1 or sensor 2). This dataset contains outliers.
}

\item {
	\textbf{University: ~\cite{University:2019}}
	This dataset contains 286 records about universities. Each record has 17 attributes including state, university name,
	SAT scores, etc. The classification task is to predict whether the
	expenses are greater than 7, 000 for each university. This dataset
	contains inconsistent representations for states and locations.
}

\item  {
	\textbf{USCensus: ~\cite{USCensusData:2019}}
	This dataset contains 32,561 US Census records
	for adults. Each record has 14 attributes including age, education,
	sex, etc. The classification goal is to predict whether the adult earns
	more than \$50,000. This dataset contains missing values.
}

\end{itemize}

\section{ML Models}

We select seven classical and competitive classification algorithms in our experiments, including \textbf{Logistic Regression, KNN,Decision Tree, Random Forest, Adaboost, XGBoost and Naive Bayes}.
We used \textbf{scikit-learn} ~\cite{Scikit-learn:2011} to train models. Each model is described
below.


\begin{itemize}
	\item { \textbf{Logistic Regression:}
	Logistic regression is a binary classifier that
	uses a Sigmoid function to create a linear classification boundary. Logistic regression uses optimization methods to determine
	the best regression coefficient of the function based on the training data. ~\cite{McFadden1973}
}

\item {
	\textbf{KNN Classifier:} \textit{KNN classifier} uses a distance metric (\textit{Euclidean
	distance} in our experiments), and the number of nearest neighbors
	(k) to calculate the distance between records in the training set.
	After calculating the distance it then retrieves the k nearest neighbors. Once the algorithm has found those neighbors, it can classify
	a record in the test set by computing its distance to other training
	records and using the class labels of the nearest neighbors to deter-
	mine the label class of the unknown record. ~\cite{HastieTibshirani}
}

\item {
\textbf{Decision Tree:} CART (Classification\& Regression Trees) decision trees were used in this analysis. During training, the decision
	tree splits the data based on homogeneity. Gini index is used to
	measure node impurity and the attribute with minimum Gini index
	is used as the split node. This algorithm recursively partitions data
	until the splitting is completed. ~\cite{Quinlan1986}
}
\item{
   \textbf{Random Forest:} Random forests are an ensemble learning method
	for classification. During training, the random forests algorithm
	constructs several decision trees and outputs an aggregated prediction (often the mode of the classes). Predictions in the test set are
	then made using this output ~\cite{CuiChen2015}.
}

\item {
	\textbf{Adaboost}: Adaboost, also known as “Adaptive boost”, is a meta-
	learning algorithm with high theoretical and empirical performance.
	It uses weak learners and transforms them into high performance
	learners by repeatedly emphasizing mispredicted instances. In ex-
	periments, the decision tree is our base learner. ~\cite{Hastie2009}
}
\item {
	\textbf{XGBoost}: XGBoost is short for “Extreme Gradient Boosting”. It is
	an implementation of gradient tree boosting system designed to be
	highly efficient and scalable. It is one of the most popular packages
	used by competitors to win ML challenges ~\cite{Guestrin2016}. We use gradient
	boosted tree as our base learner in the experiments.
}

\item {
	\textbf{Naive Bayes:} Naive Bayes predicts a class given a set of features using Bayes Theorem. This algorithm assumes independence among all attributes when the class is given ~\cite{Rish2001}.
}
\item {
	We preprocess features before training ML models following
	these common practice: \textbf{(1)} Categorical variables were encoded
	using one hot encoding. \textbf{(2)} Text embeddings were used for non-
	categorical text attributes. We computed their tf-idf matrix using
	\textbf{TfidfVectorizer} from \textbf{scikit-learn} ~\cite{Scikit-learn:2011}. 
	\textbf{(3)} Data were standardized to a mean of 0 and variance of 1. 
	\textbf{(4)} Class-imbalanced datasets were downsampled, i.e., for every observation of the minor class, we randomly sample from the major class without replacement, to make the number of the instances in the major class equal to that in
	the minor class during the training.
}
\end{itemize}


\section{Scenarios}

Given a dataset with a train/test split, and a cleaning method,
we can have different model performance depending on where the
cleaning is performed. 
Table ~\ref{tab:where-cleaning-is-performed } shows the four cases: 
\textbf{Case A} represents a model built using the original dirty training set and tested on
the original dirty test set; 
\textbf{Case B} represents a model built using the
original dirty training set and tested on the cleaned test set; 
\textbf{Case C } represents a model built using the cleaned training set and tested
on the original dirty test set; 
and \textbf{Case D} represents a model built
using the cleaned training set and tested on the cleaned test set.
\underline{\textbf{Our goal is compare two of them to evaluate how cleaning affects
model performance}},
and a chosen comparison between two cases is what we call a scenario in Table ~\ref{table:nettoyage_ml}

\begin{longtable}[c]{|c|c|c|}
	\caption{Where Cleaning is Performed}
	\label{tab:where-cleaning-is-performed}\\
	\hline
	& \textbf{Dirty Test Set} & \textbf{Cleaned Test Set} \\ \hline
	\endfirsthead
	%
	\multicolumn{3}{c}%
	{{\bfseries Table \thetable\ continued from previous page}} \\
	\endhead
	%
	\textbf{Dirty Training Set} & \cellcolor[HTML]{EFEFEF}A & \cellcolor[HTML]{EFEFEF}B \\ \hline
	\textbf{Cleaned Training Set} & \cellcolor[HTML]{EFEFEF}C & \cellcolor[HTML]{EFEFEF}D \\ \hline
\end{longtable}

\begin{longtable}[c]{|c|c|c|}
	\caption{Where Cleaning is Performed (Missing Values)}
	\label{tab:where-cleaning-is-performed-missing-values }\\
	\hline
	& \textbf{Deletion Test set} & \textbf{Imputation Test set} \\ \hline
	\endfirsthead
	%
	\multicolumn{3}{c}%
	{{\bfseries Table \thetable\ continued from previous page}} \\
	\endhead
	%
	\textbf{Deletion Training set} & \cellcolor[HTML]{EFEFEF}A & \cellcolor[HTML]{EFEFEF}B \\ \hline
	\textbf{Imputation Training set} & \cellcolor[HTML]{EFEFEF}C & \cellcolor[HTML]{EFEFEF}D \\ \hline
\end{longtable}

Do we then have a total of $ C^{2}_{4} = 6 $ scenarios ? The answer is no, and in
fact, only two of them (“BD” and “CD”) make sense as explained
as follows:

\begin{itemize}
	\item {
	Scenario “BD”. This shows the effects of cleaning dirty data in the
	training set when models are evaluated on the clean test set. This
	is reflective of the real-world model building phase, where we are
	given a dirty training set and we would like to know whether we
	need to clean the training set. Of course, in order to test whether
	cleaning training set helps, the two models need to be evaluated
	on the same cleaned test set.
	}
	\item {
	\textbf{Scenario “CD”}. This shows the effects of cleaning dirty data in the
	test set when models are trained on the clean training set. This
	is reflective of the real-world model deployment phase, where
	the model is deployed and is being used for new test data, and
	we would like to know whether cleaning incoming dirty test data
	helps with the predictive performance.
	}
	\item {
	\textbf{Scenario “AB”}. We do not compare the entries A and B because
	in real-world data cleaning, we do not consider evaluating a test
	set on a model trained with dirtiness, especially we are not interested in the performance improvement/degradation when swap-
	ping dirty with clean test sets.
	}
	\item {
	\textbf{Scenario “AC”}. The comparison between A and C is also not
	reported because in production we are not interested in the performance of models evaluated on a dirty test set. It is common
	practice to ensure the test set is clean for evaluating the model
	performance.
	}
	\item {
	\textbf{Scenario “AD” and “BC”}. The two scenarios are based on two
	different settings, which are not directly comparable.
	}
\end{itemize}

\textbf{Special Handling for Missing Values.} 
Missing values need special attention; they occur when no value is stored for a variable in an observation. We cannot train models when some data is missing. 
Thus, Cases A and B in Table ~\ref{tab:where-cleaning-is-performed} are not available for missing values. Instead of comparing dirty and cleaned datasets, 

we compare the difference between a deletion dataset (a dataset with missing values deleted) and an imputation dataset (a dataset with missing values imputed). 

The four cases for missing values are shown in Table ~\ref{tab:where-cleaning-is-performed-missing-values }. 
We only consider the “BD” scenario for missing values, which captures the difference of imputing and deleting training samples while testing the model performance in the imputed test set. 
This scenario is the authentic scenario we encounter in production, where it is not allowed to delete instances from the test set, so missing values in the test set have to be repaired using
imputation methods.

\section{Running The Benchmark}

We have defined the domain of each key attribute. We call each
valid assignment of key attributes an experiment specification. By
definition, the experiment specification for $a$ tuple $t$ in relation

is $ t[R - Flag]$, where $R \in \{R1, R2, R3\} $. For example in Table ~\ref{table:experiment-specifications} $s_{1}$, $s_{2}$ and $s_{3}$ are there experiment specifications in $R1$, $R2$ and $R3$, respectively.

\begin{table}[H]	
	\leftskip=3em
	\begin{flushleft}
		\leftskip=3em
		\textbf{$s_{1}$}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & ML Model & Scenario \\ 
		\hline 
		EEG & Outliers & IQR & Mean Imputation & Logistic Regression & BD  \\ 
		\hline 
	\end{tabular} \linebreak	
	
	\begin{flushleft}
		\leftskip=3em
		\textbf{$s_{2}$}
	\end{flushleft}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Detection & Repair & Scenario \\ 
		\hline 
		EEG & Outliers & IQR & Mean Imputation & BD  \\
		\hline 
	\end{tabular} \linebreak
	
	\begin{flushleft}
		\leftskip=3em
		\textbf{$s_{3}$}
	\end{flushleft}	
	
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		Dataset & Error Type & Scenario \\ 
		\hline 
		EEG & Outliers & BD \\ 
		\hline 
	\end{tabular} \linebreak
	\caption{Example of Experiment specifications}
	\label{table:experiment-specifications}
\end{table}

In this section, we present how to run the benchmark to generate
flags for each experiment specification in $R1$, $R2$ and $R3$. 

\subsection{Generating One Metric Pair}

Given an experiment specification in R1, we can generate a pair
of metrics through following steps:

\begin{enumerate}
	\item {
		\textbf{Split dataset.} We first split dataset randomly into a training
		and a test set with a $70/30$ ratio.
	}
	\item {
		\textbf{Clean dataset.} We clean the error in the training set and test
		set with the specific cleaning methods. To avoid the data
		leakage problem, all statistics needed for data cleaning, such
		as mean and standard deviation, are computed only on the
		training set and used to clean both training and test set.
    }
	\item {
		\textbf{Training ML models.} If the scenario is BD, we train two ML
		models, one on dirty training set and one on clean training
		set. If the scenario is \textbf{CD}, we only train one ML model on the
		clean training set. We tune hyper-parameters of ML models
		using random search and 5-fold cross validation.
	}
	\item {
		\textbf{Training ML models.} If the scenario is BD, we train two ML
		models, one on dirty training set and one on clean training
		set. If the scenario is \textbf{CD}, we only train one ML model on the
		clean training set. We tune hyper-parameters of ML models
		using random search and 5-fold cross validation.
	}
	\item {
		\textbf{Evaluating ML models.} If the scenario is \textbf{BD}, we evaluate
		two ML models (one trained on a dirty training set, anther
		trained on a clean training set) on the clean test set to get
		a pair of metrics. If the scenario is \textbf{CD}, the model trained
		on clean training set will be evaluated on dirty test set and
		clean test set respectively to get a pair of metrics. The evaluation metric is selected based on class imbalance. For class-
		imbalanced datasets \textit{(i.e., KDD and Credit)}, we use F1 score
		to evaluate the performance of models but for all the other
		datasets, we use accuracy as the evaluation metric.
	}
\end{enumerate}

For experiment specifications in $R2$, the difference is that we
train various ML models at step \textbf{(3)} and select the model with the
best validation accuracy from cross validation as the model evaluated in step \textbf{(4)}. 
For R3, in addition to model selection, we use
various cleaning methods to clean dataset at step \textbf{(2)} and select the cleaning method resulting in the best validation accuracy. At step
\textbf{(4),} the test set cleaned by the best cleaning method is used to evaluate the best model.


\begin{example}
\textit{We take the specifications in Table ~\ref{table:experiment-specifications} as an example to show how to generate one metric pair for each specification.}
\end{example}

To generate metric pair for $s_{1}$ , we first split EEG into training
and test datasets. We detect outliers in the training set and test set
using \textbf{IQR} detection and repair them with mean imputation. 
The quantiles used in detection and mean used in repair are computed on the training set. 
Then, since the scenario here is \textbf{BD}, we train two logistic regression models on a dirty training set and a cleaned training set respectively. Finally, we evaluate two models on the cleaned test set and get two test accuracy scores to form a metric pair as shown in Table ~\ref{tab:metric-pairs}


\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			& \multicolumn{2}{c|}{\textbf{Train on Dirty Training Set}} & \multicolumn{2}{c|}{\textbf{Train on Clean Training Set}} \\ \hline
			\textbf{Model} & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} \\ \hline
			\textbf{Logistic Regression} & 0.638849 & 0.634179 & 0.673467 & 0.668892 \\ \hline
			\multicolumn{5}{|c|}{\textbf{Metric Pair: (0.634179, 0.668892)}} \\ \hline
		\end{tabular}%
	}
	\caption{$s_{1}$ Metric Pairs}
	\label{tab:metric-pairs}
\end{table}

To generate the metric pair for $s_{2}$ , we train various ML models.
Table ~\ref{tab:s2_metric_pairs} shows that based on the validation accuracy, \underline{\textbf{XGBoost} is
the best model trained on the dirty training set} and \textbf{KNN} \underline{is the best
model trained on the clean training set}. We then evaluate two best
models on the cleaned test set to get two test accuracy scores to
form a metric pair.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			& \multicolumn{2}{c|}{\textbf{Train on Dirty Training Set}} & \multicolumn{2}{c|}{\textbf{Train on Clean Training Set}} \\ \hline
			Model & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} \\ \hline
			AdaBoost & 0.763205 & 0.711393 & 0.718193 & 0.715176 \\ \hline
			Decision Tree & 0.822621 & 0.754784 & 0.796487 & 0.810414 \\ \hline
			KNN & 0.895481 & 0.821095 & {\color[HTML]{FE0000} 0.948312} & {\color[HTML]{FE0000} 0.956386} \\ \hline
			Logistic Regression & 0.638849 & 0.634179 & 0.673467 & 0.668892 \\ \hline
			Naive Bayes & 0.453365 & 0.457276 & 0.634745 & 0.638407 \\ \hline
			Random Forest & 0.918556 & 0.854695 & 0.903680 & 0.903680 \\ \hline
			XGBoost & {\color[HTML]{FE0000} 0.932098} & {\color[HTML]{FE0000} 0.862706} & 0.920369 & 0.922786 \\ \hline
		\end{tabular}%
	}
	\caption{$s_{2}$ Metric Pairs}
	\label{tab:s2_metric_pairs}
\end{table}

To generate the metric pair for $s_{3}$ , in addition to model selection,
we use various cleaning methods. Table ~\ref{tab:s3_metric_pairs} shows the clean test
accuracy of best models under each cleaning methods.Based on
the validation accuracy, detecting outliers by \textbf{SD} and repairing by
deletion is the best cleaning method. Hence, we use its metric pair
as the metric pair for $s_{3}$.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|c|c|c|}
			\hline
			Detect Method & Repair Method & \begin{tabular}[c]{@{}c@{}}Validation Accuracy of \\ Best Model on\\ Clean Training set\end{tabular} & \begin{tabular}[c]{@{}c@{}}Clean Test Accuracy of\\ Best Model on\\ Dirty Training set\end{tabular} & \begin{tabular}[c]{@{}c@{}}Clean Test Accuracy of\\ Best Model on\\ Clean Training set\end{tabular} \\ \hline
			SD & Delete & {\color[HTML]{FE0000} 0.959370} & {\color[HTML]{FE0000} 0.937612} & {\color[HTML]{FE0000} 0.969928} \\ \hline
			SD & Mean Imputation & 0.955179 & 0.938140 & 0.964174 \\ \hline
			SD & Median Imputation & 0.955179 & 0.938140 & 0.964174 \\ \hline
			SD & Mode Imputation & 0.955179 & 0.937917 & 0.964174 \\ \hline
			IQR & Delete & 0.958072 & 0.929052 & 0.967190 \\ \hline
			IQR & Mean Imputation & 0.948312 & 0.862706 & 0.956386 \\ \hline
			IQR & Median Imputation & 0.944115 & 0.868046 & 0.951268 \\ \hline
			IQR & Mode Imputation & 0.946308 & 0.870049 & 0.957499 \\ \hline
			IF & Delete & 0.959250 & 0.935250 & 0.969846 \\ \hline
			IF & Mean Imputation & 0.957466 & 0.925456 & 0.966845 \\ \hline
			IF & Median Imputation & 0.957371 & 0.924789 & 0.966177 \\ \hline
			IF & Mode Imputation & 0.956990 & 0.927236 & 0.966400 \\ \hline
			\multicolumn{5}{|c|}{Metric Pair: (0.937612, 0.969928)} \\ \hline
		\end{tabular}%
	}
	\caption{$s_{3}$ Metric Pairs}
	\label{tab:s3_metric_pairs}
\end{table}

\section{Handling Randomness}
The above procedure has randomness, which mainly comes two
sources: 
(1) Search Randomness. This is introduced by random
search in tuning hyper-parameters. Different search spaces will result in different performances, which may affect the evaluation of ML models. 
(2) Split Randomness. This is introduced by random train/test split. Different train/test splits may result in different error distribution, which may affect the quality of data cleaning.

\subsection{Handling Search Randomness}
To handle the randomness from random search, we repeat step
(3) and step (4) 5 times with different seeds for random search.
Each random search will generate a pair of metrics. To aggregate
5 pairs, for specifications in $R1$, 
since we care more about the performance of a model on average, we averages each metric in the pair over 5 random searches. For specifications in $R2$ and $R3$, similarly as we select the best model based on the validation accuracy,
we select the one with the best validation accuracy from 5 random
searches for each metric. After repeating experiments with 5 times
random search, we still have one metric pair for each specification,
but it provides a better evaluation of the model performance and
reduces the effect of randomness caused by random search.

\begin{example}
	Table ~\ref{tab:aggregate-five-random-search-for-s1} shows the five metric pairs we get from repeating random search with 5 different seeds. For $s_{1}$ , we average
	over 5 random search for each metric. For $s_{2}$ , as shown in Table ~\ref{tab:aggregate-five-random-search-for-s2},
	we select the one with the best validation accuracy from 5 random
	search for each metric. $s_{3}$ can be generated in a similar way.
\end{example}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{3}{|c|}{\textbf{Train on Dirty Training Set}} & \multicolumn{2}{l|}{\textbf{Train on Clean Training Set}} \\ \hline
			\textbf{Random Search Seed} & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} & \multicolumn{1}{l|}{\textbf{Validation Accuracy}} & \multicolumn{1}{l|}{\textbf{Clean Test Accuracy}} \\ \hline
			8006 & 0.638849 & 0.634179 & 0.673467 & 0.668892 \\ \hline
			6130 & 0.638849 & 0.635292 & 0.673562 & 0.667557 \\ \hline
			5824 & 0.638849 & 0.634846 & 0.673372 & 0.668669 \\ \hline
			3659 & 0.638754 & 0.635291 & 0.672323 & 0.668892 \\ \hline
			3239 & 0.639040 & 0.634179 & 0.672323 & 0.669114 \\ \hline
			\multicolumn{2}{|c|}{Average}   & 0.634767 &  & 0.668625 \\ \hline
			\multicolumn{5}{|c|}{\textbf{Aggregated Metric Pair: (0.634767, 0.668625)}} \\ \hline
		\end{tabular}%
	}
	\caption{Aggregate Five Random Search For $s_{1}$}
	\label{tab:aggregate-five-random-search-for-s1}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{3}{|c|}{\textbf{Train on Dirty Training Set}} & \multicolumn{2}{l|}{\textbf{Train on Clean Training Set}} \\ \hline
			\textbf{Random Search Seed} & \textbf{Validation Accuracy} & \textbf{Clean Test Accuracy} & \multicolumn{1}{l|}{\textbf{Validation Accuracy}} & \multicolumn{1}{l|}{\textbf{Clean Test Accuracy}} \\ \hline
			8006 & {\color[HTML]{FE0000} 0.932098} & {\color[HTML]{FE0000} 0.862706} & {\color[HTML]{FE0000} 0.948312} & {\color[HTML]{FE0000} 0.956386} \\ \hline
			6130 & 0.930381 & 0.868046 & 0.948312 & 0.956386 \\ \hline
			5824 & 0.932098 & 0.862706 & 0.920369 & 0.922786 \\ \hline
			3659 & 0.930381 & 0.868046 & 0.948312 & 0.956386 \\ \hline
			3239 & 0.932098 & 0.862706 & 0.948312 & 0.956386 \\ \hline
			\multicolumn{5}{|c|}{\textbf{Metric Pair: (0.862706, 0.956386)}} \\ \hline
		\end{tabular}%
	}
	\caption{Aggregate Five Random Search For $s_{2}$}
	\label{tab:aggregate-five-random-search-for-s2}
\end{table}

\subsection{Handling Split Randomness}
To avoid the occasionality caused by a train/test split, we randomly split each dataset 20 times with different seeds and repeat the experiments under the same protocol on each train/test split. Each split will generate one pair of metrics. Hence, we end up with
20 metric pairs for each specification.

\begin{example}
	Table ~\ref{tab:accuracy-evulated-on-the-clean-test-set} shows 20 pairs of metrics from 20 different train/test splits for $s_{1}$.
\end{example}

\begin{table}[H]
	\centering
	\begin{tabular}{|c|l|l|c|l|l|}
		\hline
		Split Seed & \multicolumn{1}{c|}{B} & \multicolumn{1}{c|}{D} & Split Seed & \multicolumn{1}{c|}{b} & \multicolumn{1}{c|}{D} \\ \hline
		v144 & 0.632488 & 0.657321 & v5192 & 0.631954 & 0.67401 \\ \hline
		v235 & 0.634757 & 0.668625 & v5374 & 0.638362 & 0.676992 \\ \hline
		v2516 & 0.625812 & 0.666266 & v5396 & 0.641032 & 0.672452 \\ \hline
		v2895 & 0.636404 & 0.662394 & v6542 & 0.63992 & 0.670049 \\ \hline
		v2962 & 0.637161 & 0.674633 & v7751 & 0.640098 & 0.669871 \\ \hline
		v3462 & 0.644726 & 0.673654 & v7813 & 0.634535 & 0.676591 \\ \hline
		v3562 & 0.635514 & 0.67401 & v8093 & 0.636271 & 0.666489 \\ \hline
		v4225 & 0.641478 & 0.674989 & v8444 & 0.632443 & 0.673431 \\ \hline
		v4764 & 0.649177 & 0.680196 & v905 & 0.636671 & 0.673565 \\ \hline
		v5056 & 0.629773 & 0.669381 & v9394 & 0.632176 & 0.668803 \\ \hline
	\end{tabular}
	\caption{Accuracy Evaluated on the Clean Test Set}
	\label{tab:accuracy-evulated-on-the-clean-test-set}
\end{table}

Given 20 pairs of metrics for each specification $s = t[R \-- Flag]$,
we generate the flag $t[Flag]$ using paired sample $t-test$. We consider 20 metric pairs as two sets of 20 observations from the same dataset before and after data cleaning. 
Then paired sample $t-test can$ determine whether the mean difference between two sets of
observations is zero, positive or negative. The paired sample $t-test$ is formally defined below. 

For each specification $t[R\--Flag]$, let $\mu^{t}$ be the mean difference
of the metrics of the ML model before and after we clean the error
in the dataset with the detection and repair method. We define null
and alternative hypotheses for three types of paired sample $t-test$
as:

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Hypothesis}} & \multicolumn{1}{c|}{\textbf{Two-tailed t-test}} & \multicolumn{1}{c|}{\textbf{Upper-tailed t-test}} & \multicolumn{1}{c|}{\textbf{Lower-tailed t-test}} \\ \hline
	Null &  $ H^{t}_{0}:\mu^{t} = 0 $ & $H^{t}_{1}:\mu^{t} \leq 0$  & $H^{t}_{2}:\mu^{t} \geq 0$  \\ \hline
		Alternative &  $ H^{t}_{a}:\mu^{t} \neq 0 $ & $H^{t}_{b}:\mu^{t} > 0$  & $H^{t}_{c}:\mu^{t} < 0$  \\ \hline
	\end{tabular}
	\caption{Hypotheses}
	\label{tab:hypotheses}
\end{table}

We run three types of paired sample $t-test$ on 20 metric pairs. Let $p_{0}$,
$p_{1}$, $p_{2}$ be the $p-values$ of two tailed $t-test$, upper-tailed $t-test$ and
lower-tailed t-test, respectively. 

Let $\alpha$ be the significant level.The procedure for determining flags using paired sample t-test is
described below:
	

\begin{equation*}
\begin{array}{l}
   (1) \quad if \quad p_{0} \geq \alpha,\quad t[Flag] = "S" \\
   (2) \quad if \quad p_{0} < \alpha \quad \textrm{and} \quad p_{1} < \alpha, \quad t[Flag] = "P" \\
   (3) \quad if \quad p_{0} < \alpha \quad \textrm{and} \quad p_{2} < \alpha,  \quad t[Flag] = "N" \\
 \end{array}
\end{equation*}

The intricacy of conducting two-tailed test and one-tailed test
together lies in the fact that if the test statistics distribution is symmetric 
(e.g., Gaussian), the p-value in one of one-tailed tests is half
of the p-value in a two-tailed test. Hence, a two-tailed test with
significance does imply that the one-tailed test under the same distribution is also significant; yet if the one-tailed test is significant,
the two-tailed one is not necessarily significant. What is criticized
often is that people only report a one-tailed test p-value because the
two-tailed test is insignificant. However, in our case, we do not face
this claim because we conduct three tests and only report the one-
tailed test results if its corresponding two-tailed test is significant.

\begin{example}
	For $s_{1}$ , we run two-tailed, upper-tailed and lower-tailed sample t-test on 20 metric (\ref{tab:accuracy-evulated-on-the-clean-test-set}) pairs $Assume \alpha = 0.05$. Table \ref{tab:hypothesis-testing} $shows \quad p_{0} < \alpha \quad and \quad p_{1} < \alpha$. 
	Thus, the flag of $s_{1}$ is determined to be "P"
\end{example}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Type & p-value \\ \hline
		Two-Tailed $(p_{0})$& 3.82E-17 \\ \hline
		Upper-tailed $(p_{1})$ & 1.91E-17 \\ \hline
		Lower-tailed $(p_{2})$ & 1  \\ \hline
	\end{tabular}
	\caption{p-values in t-test and Hypothesis Testing}
	\label{tab:hypothesis-testing}
\end{table}

\section{Controlling False Discoveries}
Since we aim at studying the significant impacts of data cleaning techniques on ML performances in spite of the search and split randomness, we face the challenges that not all statically significant results in individual hypothesis tests are true discoveries. 
Some results are significant simply due to the large number of tests examined. 

This commonly known as the multiple hypothesis testing or the multiple comparisons problem in the statistics literature ~\cite{Rupert2012}.

To see the effect of multiple testing, consider a case where there are 20 hypotheses to test and we set a significance level of $\alpha = 0.05$. The probability of observing at least one significant result just due to chance is $ 1 - (1 - \alpha)^{20} \approx 0.64$

With just 20 test considered, we have a 64\% chance of observing at least one significant result, even if all of the tests are actually not significant.

With 3, 990, 570 and 150 hypotheses in our relations $R1$, $R2$ and $R3$, respectively, it is highly likely that our results contain many false discoveries by chance. Strategies to control the false discoveries caused by the multiple hypothesis testing problem usually adjust the significance level $\alpha$ in some way ~\cite{Rupert2012} ~\cite{Benjamini1995}.

For example a simple way  to adjust $\alpha$ is called the \textit{Bonferroni correction} ~\cite{Bonferroni1936} \footnote{Bonferroni correction is one of many familywise error rate
(FWER) methods, for an extensive survey, c.f. ~\cite{Zhao2017}} which test each hypothesis at the significance level of $\frac{\alpha}{m}$ instead of $\alpha$, where $m$ is the number of false negatives because it can miss a lot of true significant tests.

Instead of adjusting the significance level $\alpha$ for every test, another strategy is to rank the tests by their p-values, which indicate the statistical significance levels of tests ~\cite{Wasserstein2016}.

This is called the \textbf{FDR} approach ~\cite{Friedman2001} where we ensure that in expectation, $\frac{V}{R} = FDR$, where $R$ is the total number of rejections, and $V$
the number of false rejections. Common FDR approaches include Benjamini-Hochberg (BH) and Benjamini-Yekutieli (BY) procedures, where we try to control the FDR that is “(upper) bounded
by a user-defined level $\alpha$" ~\cite{Friedman2001}.We employed the BY
procedure since it controls the FDR under arbitrary dependence assumptions\footnote{https://en.wikipedia.org/wiki/False\_discovery\_rate (last accessed:July 31, 2019}
For each relation, we conduct a separate multiple hypothesis testing. We assign $\alpha$ to be 0.05 in our experiments.

\begin{example}
	P-values for $s_{1}$ Table \ref{tab:hypothesis-testing} are collected in a multiple testing setting where we run BY-Procedure on p-values of all hypotheses in $R1$. Table ~\ref{tab:hypothesis-testing-corrected-pvalues}
	shows the collected the corrected p-values for $s_{1}$. Since $p_{0} < \alpha \quad and \quad p_{1} < \alpha$. Thus, the flag of $s_{1}$ is finally determined to be "P".
\end{example}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Test-Type & Corrected p-value \\ \hline
		Two-Tailed $(p_{0})$& 6.28E-17 \\ \hline
		Upper-tailed $(p_{1})$ & 3.25E-17 \\ \hline
		Lower-tailed $(p_{2})$ & 1  \\ \hline
	\end{tabular}
	\caption{Corrected p-values of the Example Analysis (Outlier)}
	\label{tab:hypothesis-testing-corrected-pvalues}
\end{table}

\section{Analyzing Benchmark Results}
We inspect the correlation between the test accuracy scores in
the scenarios BD and CD. In Figure ~\ref{fig:Test_Accuracy_Scores_in_Scenarios_BD_and_CD_of_20_Splits_for_Five_Error_Types}
The scatter plots are generated based R3. We plot the test accuracy scores of 20 splits in each
dataset, given the best model and the best data cleaning method. Different colors correspond to various datasets. The visualization show that: 
\begin{enumerate}
	\item{
		\underline{Cleaning does not improve ML performance.}
	}
	\item {
		\underline{Cleaning can help improve accuracy scores up to 10\% (e.g., cleaning outliers in the scenario CD).}
	}
	\item {
		\underline{The improvements vary largely from one error type to another.}
	}
	\item {
		\underline{We need a systematic and principled approach to analyze the results.}
	}
\end{enumerate}

Table ... present the results according to the query templates we define in Section \ref{sec:analysis-methodology}.

In section \ref{sec:inconsistencies} to .. we present the impacts of each type of error on ML by analyzing the query results. 
In Section .. we summarize the key insights. 

\section{Inconsistencies} \label{sec:inconsistencies}

\begin{enumerate}
	\item {
		Cleaning inconsistencies is more likely to have insignificant impact and unlikely to have negative impact on ML.
	}
	\item {
		Model selection increases the probability of having positive impacts on ML.
	}
\end{enumerate}

\begin{itemize}
	\item {
		\textbf{Q1:} We first group by flags on the tuples in R1, R2 and R3. 
		Table ..:Q1(E=Inconsistencies) shows no negative flags (“N”) in the
		impact directionality. For every relation, the insignificant changes
		(“S”) have the largest likelihood. 
		This implies that cleaning inconsistency in both training and test sets is unlikely to produce negative
		impacts on the ML model performance. Furthermore, comparing the percentages of “P” among all the relations, selecting the best ML model and the best data cleaning strategy helps to gradually
		introduce positive changes to ML performances after data cleaning.
	}
	\item {
		\textbf{Q2:} Table..: Q2(E=Inconsistencies) shows the query results
		of grouping by flags and scenarios, which follows the tendency we
		observe in Q1, i.e., no negative impacts on ML performance after cleaning inconsistency in both scenarios. Adding an auto ML model/cleaning tuner increases the changes of positive impacts.
	}
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[angle=-90,scale=.3]{Test_Accuracy_Scores_in_Scenarios_BD_and_CD_of_20_Splits_for_Five_Error_Types}
	\caption{Test Accuracy Scores in Scenarios BD and CD of 20 Splits for Five Error Types}
	\label{fig:Test_Accuracy_Scores_in_Scenarios_BD_and_CD_of_20_Splits_for_Five_Error_Types}
\end{figure}
