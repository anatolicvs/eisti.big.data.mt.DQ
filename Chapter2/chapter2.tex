%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************
\chapter{Data Quality Dimensions}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


More specifically, quality dimensions can refer either to the extension of data, i.e., to data values, or to heir 
intension, i.e., to their schema. Both data dimensions are usually defined in qualitative way, referring to general properties of data and schemas,
and and related definitions do not provide any facility for assigning values to dimensions themselves. 

Specifically, definitions do not provide quantitative measures, and one or more metrics are to be associated with dimensions as separate, distinct 
properties. For each metric, one or more measurement methods are to be provided regarding (i) where the measurement is taken, (ii) what data are included, 
(iii) the measurement device, and (iv) the scale on which results are reported.

According to the literature, at times we will distinguish between dimensions and metrics, while other times we will directly metrics.

\begin{table}[H]
\caption{Data Quality Dimensions I}
\centering
\begin{tabular}{p{4.0cm} p{10cm}}
\toprule
\textbf{Dimension Name} & \textbf{Description} \\ 
\bottomrule
Data Governance & 
Do organization-wide data standards exist and are they enforced? Do clearly
defined roles and responsibilities exist for data quality related activities? Does
data governance strive to acquire and maintain high-quality data through proactive
management? \\
Data Specifications &
Are data standards documented in the form of a data dictionary, 
data models, meta data, and integrity constraints?  
\\
Data Integrity & 
How is data integrity maintained? How are data integrity violations detected and
resolved ?   
\\
Data Consistency & 
If data redundancy exists, how is data consistency achieved? What methods
are used to bring consistency to data that has become inconsistent? If data is
geographically replicated, how is the consistency and latency managed? 
\\
Data Currency & 
Is the data current? Do procedures exists to keep the data current and purge stale
data? 
\\
Data Duplication & Are there effective procedures in place to detect and remove duplicate data? 
\\
Data Completeness & 
Is the data about entities complete? How is missing data managed? 
\\
Data Provenance & 
Is a historical record of data and its origination maintained? If the data is acquired
through multiple sources and has undergone cleaning and transformations, does
the organization maintain a history of all changes to the data? 
\\
Data Heterogeneity & 
If multi-modality data about an entity is available, is that data captured and used? 
\\
Streaming Data & 
How is streaming data sampled, filtered, stored, and managed for both real-time
and batch processing? 
\\
Outliers & 
How are outliers detected and addressed? Are there versions of datasets that
are outlier-free? Does each version correspond to a different method for outlier
detection and treatment? 
\\
Dimensionality &
Reduction Do the datasets feature dimensionality reduced versions? How many versions are
available? \\
Feature Selection & 
Do datasets have versions that exclude features that are either redundant, highly
correlated, or irrelevant? How many versions are available? 
\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
% \vspace*{-3.5in}
\caption{Data Quality Dimensions II}
\centering
\begin{tabular}{p{4.0cm} p{10cm}}
\toprule
\textbf{Dimension Name} & \textbf{Description} \\ 
\bottomrule
Feature Extraction & 
Do the datasets provide a set of derived features that are informative and non-
redundant, in addition to the original set of variables/features? How many such
derived feature sets are available? 
\\
Business Rules & 
Does a process exist to identify, refine, consolidate, and maintain business
rules that pertain to data quality? Do rules exist to govern data cleaning and
transformations, and integrating related data of an entity from multiple sources?
What business rules govern substitutions for missing data, deleting duplicate data,
and archiving historical data? Are there rules for internal data audit and regulatory
compliance? \\
Data Accuracy & 
Data can be syntactically accurate and yet semantically inaccurate. For example,
a customer's mailing address may meet all the syntactic patterns specified by the
postal service, yet it can be inaccurate. How does the organization establish the
accuracy of data? \\
Gender Bias & 
Is the data free from factors that lead to gender bias in machine learning
algorithms? \\
Confidentiality and Privacy & 
Are procedures and controls implemented for data encryption, data de-
identification and re-identification, and differential privacy? \\
Availability and Access Controls & 
How is high data availability achieved? What security controls are implemented to
protect data from unauthorized access? How are user entitlements to data access
and modifications defined and implemented? \\
\bottomrule
\end{tabular}
\end{table}


\section{Accuracy}

Accuracy ~\citep{Falorsi} is defined as the closeness between a value v and a value v' , considered as 
the correct representation of the real-life phenomenon that v aims to
represent. As an example if the name of a person is John, the value v' = AytaÃ§
is correct, while the value v = Ayt is incorrect. Two kinds of accuracy can be
identified, namely a syntactic accuracy and a semantic accuracy.

Let us consider a relation schema \textbf{R} consisting of \textbf{K}
attributes and a relational table \textbf{r} consisting of N tuples. 

Let $q_{ij}(i=1..N, j=1..K)$ be a boolean variable defined to correspond to the cell values 
$y_{ij}$, is syntactically accurate, while otherwise it is equal to 1. 

In order to identify whether or not accuracy errors affect a matching of 
relational table \textbf{r} with a reference table \textbf{r'} containing correct
values, we introduce a further boolean variable $s_i$ equal to 0 if the tuple $t_i$ 
matches a tuple in \textbf{r'}, and otherwise equal to 1. We can introduce three metrics to distinguish
the relative importance of value accuracy in context of the tuple.
The first two metrics have the purpose of giving a different importance to 
errors on attributes that have a higher identification power, in line with 
the above discussion.

The first metric called \textit{weak accuracy error,} and is defined:

\begin{equation*}
    \sum_{i = 1}^{N} \frac{\beta(q_i > 0)\wedge(s_i=0)}{N}
\end{equation*}

where $\beta(.)$ is a boolean variable equal to 1 if the condition in parentheses is 
\textit{true}, 0 otherwise, and $q_i =\sum\nolimits_{j=1}^K q_{ij}$. Such metric considers the case in 
which for a tuple $t_i$ accuracy errors occur $(q_i > 0)$ but do not affect identification
$(s_i = 0)$.

The second metric is called \textit{strong accuracy error}, and is defined assigning

\begin{equation*}
    \sum_{i = 1}^{N} \frac{\beta(q_i > 0)\wedge(s_i=1)}{N}
\end{equation*}

where $\beta(.)$ and $q_i$ have the same meaning as above. Such a metric considers
the case which accuracy errors occur $(q_i > 0)$ for a tuple $t_i$ and actually 
do affect identification $(s_i = 1)$.

The third metric gives the percentage of accurate tuples matched with
the reference table. It is expressed by the degree of syntactic accuracy of the
relational instance \textbf{r}

\begin{equation*}
    \sum_{i = 1}^{N} \frac{\beta(q_i =0)\wedge(s_i=0)}{N}
\end{equation*}

by actually considering the fraction of accurate $(q_i = 0)$ matched $(s_i =0)$ tuples.

\section{Completeness}
Completeness can be generically defined as the extent to which data are
of sufficient breadth, depth, and scope for the task at hand ~\citep{Wang} \citep{Jaro1985}
